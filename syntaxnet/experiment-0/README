This directory contains CONLL files generated by SyntaxNet default
parser for Portuguese.  To reproduce, clone the SyntaxNet repository
from:

https://github.com/tensorflow/models

Next, download the Portuguese trained model, available from:

https://github.com/tensorflow/models/blob/master/syntaxnet/universal.md

Next, you need to extract the sentences from the DHBB files, since the
SyntaxNet default parser only deals with sentences, one per line.  You
can use the sentence-pt.py script in this directory that uses FreeLing
4.0 sentence splitter for Portuguese (make sure you update it with the
proper paths of your local FreeLing installation).

Next, just run syntaxnet over each sentence file as follows:

cat <file> | <repo>/syntaxnet/models/syntaxnet/syntaxnet/models/parsey_universal/tokenize.sh <lang-repo>/Portuguese | <repo>/syntaxnet/models/syntaxnet/syntaxnet/models/parsey_universal/parse.sh <lang-repo>/Portuguese

Where <repo> is the root directory where you cloned syntaxnet into and
<lang-repo> is where you downloaded the Portuguese model.

Notice that in both cases we are using the default scripts supplied by
the syntaxnet project, but nothing prevents us from using the
syntaxnet parser directly if needed.

To find entities in the CONLL files, first we need to take the
dictionaries and tokenize them by passing them through just the
"tokenize.sh" script from the line above.

File dictionary.csv contains the dictionary already tokenized and
ready to be used by search-entities.lisp.

Directory conll+entities contains the CONLL files augmented with
entity information.
